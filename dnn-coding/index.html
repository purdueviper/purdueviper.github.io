<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Advances In Video Compression System Using Deep Neural Network: A Review and Case Studies</title>

    <link rel="stylesheet" href="./css/normalize.css">
    <link rel="stylesheet" href="./css/project-page.css">

    <script src="./main.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  </head>
  <body>
    <div class="wrapper">
      <h1 class="center">Advances In Video Compression System Using Deep Neural Network:
        <br>A Review And Case Studies</h1>

      <p class="center">
        Dandan Ding, Zhan Ma, Di Chen, Qingshuang Chen, Zoe Liu and Fengqing Zhu
        <!--
        <a href="https://jspan.github.io/" target="_blank">Jinshan Pan</a>,
        <a href="https://baihaoran.xyz/about" target="_blank">Haoran Bai</a>,
        and Jinhui Tang
        -->
      </p>

      <p class="center"><img src=./sec1_introduction_contribution.jpg alt="Figure 1. <b>The main content of this paper</b> where advances in DNN video coding and processing are reviewed and surveyed. Furthermore, we proposed our solutions for pre-processing, video coding, and post-processing, respectively, as case studies."></p>

      <h2>Abstract</h2>
      <hr>
      <p>
         Significant advances in video compression system have been made in the past several decades to satisfy the nearly exponential growth of Internet-scale video traffic. From the application perspective, we have identified three major functional blocks including pre-processing, coding, and post-processing, that have been continuously investigated to maximize the end-user quality of experience (QoE) under a limited bit rate budget. Recently, artificial intelligence (AI) powered techniques have shown great potential to further increase the efficiency of the aforementioned functional blocks, both individually and jointly. In this article, we review extensively recent technical advances in video compression system, with an emphasis on deep neural network (DNN)-based approaches; and then present three comprehensive case studies. On pre-processing, we show a switchable texture-based video coding example that leverages DNN-based scene understanding to extract semantic areas for the improvement of subsequent video coder. On coding, we present an end-to-end neural video coding framework that takes advantage of the stacked DNNs to efficiently and compactly code input raw videos via fully data-driven learning. On post-processing, we demonstrate two neural adaptive filters to respectively facilitate the in-loop and post filtering for the enhancement of compressed frames. Finally, a companion website hosting the contents developed in this work can be accessed publicly at <a href="https://purdueviper.github.io/dnn-coding/">https://purdueviper.github.io/dnn-coding/</a>.
      </p>

      <h2>Downloads</h2>
      <hr>
      <p>
        <ul>
          <li><a href="" target="_blank">Paper</a></li>
          <li><a href="" target="_blank">Supplementary</a></li>
          <li>
		Source Code
		<ul>
		<li><a href="https://lorenz.ecn.purdue.edu/~zhu0/dnn-coding/code.zip
" target="_blank">Pre-processing</a></li>
                <li><a href="https://njuvision.github.io/Neural-Video-Coding" target="_blank">Coding</a></li>
		<li><a href="https://github.com/IVC-Projects" target="_blank">Post-processing</a></li>
		</ul>
	</li>
        </ul>
      </p>
<!--
      <div class="fl m3">
        <h4>Downloads</h4>
        <ul>
          <li><a href="https://arxiv.org/abs/2004.02501" target="_blank">Paper</a></li>
          <li><a href="https://drive.google.com/drive/folders/1lw_1jITafEQ9DvMys_S6aYwtNApYKWsz?usp=sharing" target="_blank">Supplementary</a></li>
          <li><a href="https://github.com/csbhr/CDVD-TSP" target="_blank">Code</a></li>
        </ul>
      </div>

      <div class="fl m7">

        <h4>Citation</h4>
        <pre>@InProceedings{Pan_2020_CVPR,<br>  author = {Pan, Jinshan and Bai, Haoran and Tang, Jinhui},<br>  title = {Cascaded Deep Video Deblurring Using Temporal Sharpness Prior},<br>  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},<br>  month = {June},<br>  year = {2020}<br>}</pre>
      </div>
      <hr>

      <h2>Proposed Algorithm</h2>
      <hr>
      <p class="center"><img src="https://s1.ax1x.com/2020/03/31/GQutu8.png" alt="Supp-Figure 1. An overview of the proposed method at one stage."></p>
      <p>
        The proposed algorithm contains the optical flow estimation module, latent image restoration module, and the temporal sharpness prior. For the optical flow estimation, we use the PWC-Net [5] to estimate optical flow. For the latent image restoration module, we use an encoder-decoder architecture based on [26]. However, we do not use the ConvLSTM module. Other network architectures are the same as [26].
      </p>
      <p>
        To effectively train the proposed algorithm, we develop a cascaded training approach and jointly train the proposed model in an end-to-end manner. At each stage, it takes three adjacent frames estimated from the previous stage as the input and generates the deblurred results of the central frame. When handling every three adjacent frames, the proposed network shares the same network parameters.
        The variables \( \tilde{I}_{i+1}(x) \) and \( \tilde{I}_{i-1}(x) \) denote the warped results of \( I_{i+1}(x+u_{i+1\rightarrow i}) \) and \( I_{i-1}(x+u_{i-1\rightarrow i}) \), respectively.
      </p>
      <h3>Temporal sharpness prior</h3>
      <p>
        As demonstrated in [4], the blur in the video is irregular, and thus there exist some pixels that are not blurred. Following the conventional method [4], we explore these sharpness pixels to help video deblurring. The sharpness prior is defined as:
        \[ S_i(x) = exp(-\frac{1}{2} \sum_{j\&j\neq0}D(I_{i+j}(x+u_{i+j \rightarrow i}); I_i(x))) \tag{10} \]
        where \( D(I_{i+j}(x+u_{i+j \rightarrow i}); I_i(x)) \) is defined as \( \left \| I_{i+j}(x+u_{i+j \rightarrow i} - I_i(x) \right \|^2 \).
      </p>
      <p>
        Based on (10), if the value of \( S_i(x) \) is close to 1, the pixel \(x\) is likely to be clear. Thus, we can use \( S_i(x) \) to help the deep neural network to distinguish whether the pixel is clear or not so that it can help the latent frame restoration. To increase the robustness of \( S_i(x) \), we define \( D(.) \) as:
        \[ D(I_{i+j}(x+u_{i+j \rightarrow i}); I_i(x)) = \sum_{y\in \omega(x)} \left \| I_{i+j}(x+u_{i+j \rightarrow i} - I_i(x) \right \|^2 \tag{11} \]
        where \( \omega(x) \) denotes an image patch centerd at pixel \( x \).
      </p>

      <h2>Quantitative Results</h2>
      <hr>
      <p>
        We further train the proposed method to convergence, and get higher PSNR/SSIM than the result reported in the paper.
      </p>
      <p>
        Quantitative results on the benchmark dataset by Su et al. [24]. All the restored frames instead of randomly selected 30 frames from each test set [24] are used for evaluations. <em>Note that: Ours* is the result that we further trained to convergence, and Ours is the result reported in the paper.</em>
      </p>
      <p class='center'><img src="https://s1.ax1x.com/2020/03/31/GQOAv6.png" alt="Table 1. Quantitative evaluations on the video deblurring dataset [24] in terms of PSNR and SSIM."></p>
      <p>
        Quantitative results on the GOPRO dataset by Nah et al.[20].
      </p>
      <p class='center'><img src="https://s1.ax1x.com/2020/03/31/GQYZi8.png" alt="Table 2. Quantitative evaluations on the video deblurring dataset [20] in terms of PSNR and SSIM."></p>
-->

<!--
      <p class='center'><img src="https://s1.ax1x.com/2020/03/31/GQt01g.png" alt="Figure 2. Deblurred results on the test dataset [24]."></p>
      <p class='center'><img src="https://s1.ax1x.com/2020/03/31/GQtgA0.png" alt="Figure 3. Deblurred results on the test dataset [20]."></p>
-->

      <h2>Visual Comparisons</h2>
      <hr>
      <p>Sample reconstructed videos in Section V，Table II. The AV1 baseline results are prefixed by 'av1_', and the switchable texture mode results are prefixed by 'switch_'.</p>
      <p id="back"></p>
      <ul id="videos">
      </ul>

      <h2>Contact</h2>
      <hr>
      <p>Please address all correspondence to:</p>
      <ul>
        <li>E-mail: <a href="mailto:zhu0@purdue.edu">zhu0@purdue.edu</a></li>
      </ul>

      <h2>Acknowledgment</h2>
      <hr>
      <p>
        This work was sponsored by a Google Faculty Research Award and the Google Chrome University Research Program.
        The webpage template was inspired by <a href="https://baihaoran.xyz/projects/cdvd-tsp/index.html" target="_blank">this project page</a>.
      </p>
<!--
      <h2>References</h2>
      <hr>
      <div>[1] Miika Aittala and Frédo Durand. Burst image deblurring using permutation invariant convolutional neural networks. In ECCV, pages 748–764, 2018. 2</div>
-->
    </div>

  </body>
</html>
